{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: solid blue 2px; padding: 15px; margin: 10px\">\n",
    "  <b>Overall Summary of the Project ‚Äì Final Iteration</b><br><br>\n",
    "\n",
    "  Hi Taylor, I‚Äôm <b>Victor Camargo</b> (<a href=\"https://hub.tripleten.com/u/e9cc9c11\" target=\"_blank\">TripleTen Hub profile</a>). Thanks for your thoughtful final submission. I‚Äôve reviewed your project and I‚Äôm happy to say it is now <b>approved</b>! üéâ<br><br>\n",
    "\n",
    "  <b>Highlights from your work:</b><br>\n",
    "  ‚úîÔ∏è Cleaned and prepared the data carefully, handling types and missing values correctly.<br>\n",
    "  ‚úîÔ∏è Engineered the <b>charge_per_month</b> feature, which gave your model a stronger signal.<br>\n",
    "  ‚úîÔ∏è Built a solid training workflow with train/validation/test splits, avoiding data leakage.<br>\n",
    "  ‚úîÔ∏è Tuned thresholds and hyperparameters effectively with LightGBM to reach strong performance.<br>\n",
    "  ‚úîÔ∏è Reflected deeply on your process ‚Äî not just what worked, but also the mistakes and restarts that taught you how to structure real projects more clearly.<br><br>\n",
    "\n",
    "  <b>Final note:</b><br>\n",
    "  Your final report really stood out because of your honesty and reflection. You didn‚Äôt just show the code and metrics, you shared the journey, the restarts, the long hours, and what you learned about <b>thinking like a data scientist</b>. That growth mindset will take you very far. Your final model (LightGBM, ROC-AUC ‚âà 0.87 on test) is excellent, but even more impressive is how you described the shift from following to figuring things out on your own.<br><br>\n",
    "\n",
    "  <b>Congratulations on completing this milestone!</b> I wish you the very best in your journey ahead, keep building, keep experimenting, and keep that curiosity alive. You‚Äôve shown you have what it takes. üöÄüí™\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Project: Solution Report\n",
    "\n",
    "### What steps of the plan were performed and what steps were skipped (explain why)?\n",
    "\n",
    "All steps in the project plan were completed:\n",
    "\n",
    "- Data was loaded and cleaned. I handled missing values, corrected data types (such as converting `TotalCharges` to numeric), and verified that categorical features were encoded properly.\n",
    "- Feature engineering included creating a useful metric, `charge_per_month`, to reflect how much a customer pays on average each month.\n",
    "- I used a 3-way split (train, validation, test) to ensure proper model tuning and final evaluation.\n",
    "- Several models were trained and compared, with LightGBM selected as the final model.\n",
    "- Threshold tuning was done using validation set predictions to find the best classification cutoff for F1 score.\n",
    "- Final evaluation was performed only on the test set after all tuning was completed.\n",
    "\n",
    "No steps were skipped.\n",
    "\n",
    "---\n",
    "### What difficulties did you encounter and how did you manage to solve them?\n",
    "\n",
    "This project pushed me to slow down and rethink my process. I had to restart my notebook twice due to disorganized code and broken logic. Early on, I ran into issues with inconsistent variable names, rerunning cells out of order, and splitting the data incorrectly. At one point, I realized my model was unintentionally using test data during threshold tuning, so I had to rebuild the entire pipeline from scratch to ensure everything was clean and correct.\n",
    "\n",
    "Over the course of three days, I put in roughly 24 hours of focused work. I leaned heavily on outside tools and references to guide me, including GitHub repos for best practices, Claude and ChatGPT for clean code structuring, and documentation sites like the official Python and scikit-learn pages. I also used Dot and Google frequently to double-check method behavior and industry-standard practices. \n",
    "\n",
    "The hardest part was staying consistent and not cutting corners. Every fix taught me something, not just about syntax, but about how real projects should be structured to avoid chaos later. Once I restructured things properly, the code became more readable, and debugging became a lot easier.\n",
    "\n",
    "---\n",
    "\n",
    "### What were some of the key steps to solving the task?\n",
    "\n",
    "- Creating `charge_per_month` helped the model capture useful billing behavior.\n",
    "- Using LightGBM with GridSearchCV allowed me to explore a wide range of hyperparameters efficiently.\n",
    "- Threshold tuning on the validation set helped improve the F1 score and balance precision and recall.\n",
    "- Maintaining a clear separation between train, validation, and test sets prevented data leakage and gave a more realistic measure of performance.\n",
    "\n",
    "---\n",
    "\n",
    "### What is your final model and what quality score does it have?\n",
    "\n",
    "The final model is a LightGBM classifier with the following hyperparameters:\n",
    "\n",
    "```\n",
    "subsample: 0.65\n",
    "scale_pos_weight: 2.7\n",
    "reg_lambda: 0.7\n",
    "reg_alpha: 0.4\n",
    "num_leaves: 15\n",
    "n_estimators: 1000\n",
    "min_split_gain: 0.6\n",
    "min_child_samples: 5\n",
    "max_depth: 8\n",
    "learning_rate: 0.025\n",
    "colsample_bytree: 0.55\n",
    "```\n",
    "\n",
    "**Final Results:**\n",
    "\n",
    "- Validation ROC-AUC: 0.8652  \n",
    "- Test ROC-AUC: 0.8701  \n",
    "- Test Accuracy: 0.7083  \n",
    "- Train ROC-AUC: 0.9295  \n",
    "- Best F1 threshold: Tuned on validation set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Thoughts\n",
    "\n",
    "This entire bootcamp journey with TripleTen has been one of the most challenging things I've ever committed to. The pace, the depth of the material, and the constant demand to think critically have really stretched me. But through that, I‚Äôve started learning how to approach problems like a data scientist, not just by following tutorials, but by figuring things out when I get stuck, experimenting, and making sense of things piece by piece.\n",
    "\n",
    "I‚Äôm still learning. There‚Äôs a long way to go. But I‚Äôm starting to see how to organize, see what the project wants, and work through complex projects on my own, without needing someone to walk me through every step. That shift, from following to thinking, is what I‚Äôm most proud of so far.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
